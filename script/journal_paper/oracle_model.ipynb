{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.utils as utils\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"pems-bay\"\n",
    "val_rate = 0.20\n",
    "test_rate = 0\n",
    "n_his = 12\n",
    "n_pred = 12\n",
    "batch_size = 32\n",
    "\n",
    "end_of_initial_data_index = 29792\n",
    "data_per_step = 140\n",
    "\n",
    "dataset_path = f\"../../data/{dataset}/vel.csv\"\n",
    "adj_path = f\"../../data/{dataset}/adj.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(len_train, len_val):\n",
    "    # load dataset from .csv file\n",
    "    vel = pd.read_csv(dataset_path)\n",
    "\n",
    "    train = vel[: len_train] # if metr_la (len_train = 23991), get 23991 from dataset for training\n",
    "    val = vel[len_train: len_train + len_val] # if metr_la(23991 & 5140), get validation dataset from 23991-29131\n",
    "    test = vel[len_train + len_val:] # if metr_la, get test dataset from 29131 to the final value\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transform(data, n_his, n_pred):\n",
    "    n_vertex = data.shape[1] # number of nodes\n",
    "    len_record = len(data)\n",
    "    num = len_record - n_his - n_pred # number of sequences\n",
    "\n",
    "    if num <= 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Init a NumPy array with 0s, representing input data tensor with 4-dimensions\n",
    "    # if default (for training): [23976 x 1 x 12 x 207] - is 1 number of node features(???)\n",
    "    x = np.zeros([num, 1, n_his, n_vertex])\n",
    "    # Init a NumPy array with 0s, representing target data tensor with 2-dimensions\n",
    "    # if default (for training): [23976 x 207]\n",
    "    y = np.zeros([num, n_vertex])\n",
    "    \n",
    "    # loop over each sequence and change values of 1st dimension for both x and y tensor\n",
    "    for i in range(num):\n",
    "        head = i # define start of each sequence\n",
    "        tail = i + n_his # define end of each sequence\n",
    "        # data[head: tail] - get data from head to tail (0-12, 1-13, etc.)\n",
    "        # Example (1st iteration): head = 0, tail = 12, so extract data from 0:12\n",
    "        # Reshape that 2D matrix into a 3D one with dimensins [1, 12, 207]\n",
    "        x[i, :, :, :] = data[head: tail].reshape(1, n_his, n_vertex)\n",
    "        # Change values of 1st dimensions\n",
    "        y[i] = data[tail + n_pred - 1]\n",
    "\n",
    "    return torch.Tensor(x), torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_preprocess_2d_data(train, val, test):\n",
    "    # dimensions for train, val and test\n",
    "    num_time_sequence_train, num_nodes_train = train.shape\n",
    "    num_time_sequence_val, num_nodes_val = val.shape\n",
    "    num_time_sequence_test, num_nodes_test = test.shape\n",
    "\n",
    "    # Shape train, val, and test from 2D (time_sequence, num_nodes) to 2D (time_sequence * num_nodes, 1)\n",
    "    train_shaped = train.reshape(-1, 1)\n",
    "    val_shaped = val.reshape(-1, 1)\n",
    "    test_shaped = test.reshape(-1, 1)\n",
    "\n",
    "    # Reshape train, val and test from 2D (time_sequence * num_nodes, 1) back to 2D (time_sequence, num_nodes)\n",
    "    train = train_shaped.reshape(num_time_sequence_train, num_nodes_train)\n",
    "    val = val_shaped.reshape(num_time_sequence_val, num_nodes_val)\n",
    "    test = test_shaped.reshape(num_time_sequence_test, num_nodes_test)\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_iter_for_online(self, epoch, x_train, y_train):\n",
    "    if epoch == 0:\n",
    "        inital_x_train = x_train[:self.end_of_initial_data_index]\n",
    "        inital_y_train = y_train[:self.end_of_initial_data_index]\n",
    "        train_data = utils.data.TensorDataset(inital_x_train, inital_y_train)\n",
    "        train_iter = utils.data.DataLoader(dataset=train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        return train_iter\n",
    "    else:\n",
    "        current_train = self.train_dataset[:self.end_of_initial_data_index + (self.data_per_step * (epoch - 1))]\n",
    "\n",
    "        new_train_datastep = self.train_dataset[\n",
    "            self.end_of_initial_data_index + (self.data_per_step * (epoch - 1)):\n",
    "            self.end_of_initial_data_index + (self.data_per_step * (epoch))\n",
    "        ]\n",
    "\n",
    "        random_sample_size = (self.batch_size - 1) * self.data_per_step\n",
    "        # Randomly sample indices from current_train\n",
    "        if len(current_train) > random_sample_size:\n",
    "            random_indices = np.random.choice(current_train.shape[0], random_sample_size, replace=False)\n",
    "        else:\n",
    "            # If current_train has fewer than the required samples, take all of it\n",
    "            random_indices = current_train\n",
    "        new_train_datastep = [self.train_dataset[idx] for idx in range(len(new_train_datastep))]\n",
    "\n",
    "        new_x_train = x_train[\n",
    "            self.end_of_initial_data_index + (self.data_per_step * (epoch - 1)):\n",
    "            self.end_of_initial_data_index + (self.data_per_step * (epoch))\n",
    "        ]\n",
    "        sampled_x_train = x_train[random_indices, :]\n",
    "        new_x_train = torch.cat((sampled_x_train, new_x_train), dim=0)\n",
    "        new_y_train = y_train[\n",
    "            self.end_of_initial_data_index + (self.data_per_step * (epoch - 1)):\n",
    "            self.end_of_initial_data_index + (self.data_per_step * (epoch))\n",
    "        ]\n",
    "        sampled_y_train = y_train[random_indices, :]\n",
    "        new_y_train = torch.cat((sampled_y_train, new_y_train), dim=0)\n",
    "\n",
    "        train_data = utils.data.TensorDataset(new_x_train, new_y_train)\n",
    "        train_iter = utils.data.DataLoader(dataset=train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        return train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_congestion_alpha_propagation(\n",
    "    v,                 # speeds [T, N] (numpy array or torch.Tensor)\n",
    "    alpha=0.5,\n",
    "    vhat=None,         # per-sensor medians [N]; if None, computed from v\n",
    "    use_nanmedian=True\n",
    "):\n",
    "    # to numpy\n",
    "    try:\n",
    "        import torch\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            v = v.detach().cpu().numpy()\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    if v.ndim == 1:\n",
    "        v = v[:, None]\n",
    "    elif v.ndim > 2:\n",
    "        T = v.shape[0]\n",
    "        v = v.reshape(T, -1)\n",
    "\n",
    "    T, N = v.shape\n",
    "\n",
    "    # per-sensor medians\n",
    "    if vhat is None:\n",
    "        vhat = (np.nanmedian(v, axis=0) if use_nanmedian else np.median(np.nan_to_num(v, nan=0.0), axis=0))\n",
    "    else:\n",
    "        vhat = np.asarray(vhat, dtype=np.float32)\n",
    "        assert vhat.shape == (N,), f\"vhat must be shape (N,), got {vhat.shape}\"\n",
    "\n",
    "    thr      = alpha * vhat            # [N]\n",
    "    thr_row  = thr[None, :]            # [1, N]\n",
    "\n",
    "    # Rule 1: local threshold\n",
    "    base = (v < thr_row)               # [T, N]\n",
    "\n",
    "    # Rule 2: spatial continuity (both neighbors under their own thresholds)\n",
    "    spatial = np.zeros_like(base, dtype=bool)\n",
    "    if N >= 3:\n",
    "        left  = v[:, :-2] < thr_row[:, :-2]\n",
    "        right = v[:,  2:] < thr_row[:,  2:]\n",
    "        spatial[:, 1:-1] = left & right\n",
    "\n",
    "    # Rule 3: temporal continuity (both time neighbors under the same sensor’s threshold)\n",
    "    temporal = np.zeros_like(base, dtype=bool)\n",
    "    if T >= 3:\n",
    "        prev_t = v[:-2, :] < thr_row\n",
    "        next_t = v[ 2:,  :] < thr_row\n",
    "        temporal[1:-1, :] = prev_t & next_t\n",
    "\n",
    "    congested = base | spatial | temporal\n",
    "    return congested, vhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_congestion_alpha_with_oracle(\n",
    "    data_iter,\n",
    "    alpha=0.5,\n",
    "    vhat=None,\n",
    "    oracle_mode_type=\"perfect\",  # \"perfect\" | \"worst\"\n",
    "    perfect_noise_abs=3.06,\n",
    "    side_margin=0.1,\n",
    "):\n",
    "    import numpy as np\n",
    "    sum_abs_error = 0.0\n",
    "    sum_sq_error  = 0.0\n",
    "    sum_gt        = 0.0\n",
    "    total_preds   = 0\n",
    "\n",
    "    TP = FP = TN = FN = 0\n",
    "    total_points = 0\n",
    "    total_gt_cong = 0\n",
    "    total_pred_cong = 0\n",
    "\n",
    "    used_vhat = vhat\n",
    "\n",
    "    for item in data_iter:\n",
    "        y = item[-1] if isinstance(item, (tuple, list)) else item\n",
    "        try:\n",
    "            import torch\n",
    "            if isinstance(y, torch.Tensor):\n",
    "                y_np = y.detach().cpu().numpy()\n",
    "            else:\n",
    "                y_np = np.asarray(y)\n",
    "        except ImportError:\n",
    "            y_np = np.asarray(y)\n",
    "        y_np = np.asarray(y_np, dtype=np.float32)\n",
    "\n",
    "        if y_np.ndim == 1:\n",
    "            y_np = y_np[:, None]\n",
    "        elif y_np.ndim > 2:\n",
    "            T = y_np.shape[0]\n",
    "            y_np = y_np.reshape(T, -1)\n",
    "        T, N = y_np.shape\n",
    "\n",
    "        gt_mask, used_vhat = detect_congestion_alpha_propagation(y_np, alpha=alpha, vhat=used_vhat)\n",
    "\n",
    "        thr      = alpha * used_vhat\n",
    "        thr_row  = thr[None, :]\n",
    "        thr_full = np.broadcast_to(thr_row, y_np.shape)\n",
    "\n",
    "        if oracle_mode_type == \"perfect\":\n",
    "            base_gt = (y_np < thr_full)\n",
    "            noise = np.random.uniform(-perfect_noise_abs, perfect_noise_abs, size=y_np.shape).astype(np.float32)\n",
    "            y_pred = y_np + noise\n",
    "\n",
    "            if np.any(base_gt):\n",
    "                target_lo = thr_full - side_margin\n",
    "                y_pred[base_gt] = np.minimum(y_pred[base_gt], target_lo[base_gt])\n",
    "\n",
    "            if np.any(~base_gt):\n",
    "                target_hi = thr_full + side_margin\n",
    "                y_pred[~base_gt] = np.maximum(y_pred[~base_gt], target_hi[~base_gt])\n",
    "\n",
    "        elif oracle_mode_type == \"worst\":\n",
    "            y_pred = y_np.copy()\n",
    "            base_gt = (y_np < thr_full)\n",
    "            if np.any(base_gt):\n",
    "                target_hi = thr_full + side_margin\n",
    "                y_pred[base_gt] = target_hi[base_gt]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown oracle_mode_type: {oracle_mode_type}\")\n",
    "\n",
    "        d = np.abs(y_np - y_pred)\n",
    "        sum_abs_error += float(np.sum(d))\n",
    "        sum_sq_error  += float(np.sum(d ** 2))\n",
    "        sum_gt        += float(np.sum(y_np))\n",
    "        total_preds   += d.size\n",
    "\n",
    "        pred_mask, _ = detect_congestion_alpha_propagation(y_pred, alpha=alpha, vhat=used_vhat)\n",
    "\n",
    "        gt = gt_mask.reshape(-1)\n",
    "        pr = pred_mask.reshape(-1)\n",
    "\n",
    "        tp = int(np.sum(gt & pr))\n",
    "        tn = int(np.sum(~gt & ~pr))\n",
    "        fp = int(np.sum(~gt & pr))\n",
    "        fn = int(np.sum(gt & ~pr))\n",
    "\n",
    "        TP += tp; TN += tn; FP += fp; FN += fn\n",
    "        total_points   += gt.size\n",
    "        total_gt_cong  += int(np.sum(gt))\n",
    "        total_pred_cong+= int(np.sum(pr))\n",
    "\n",
    "    MAE   = (sum_abs_error / total_preds) if total_preds > 0 else 0.0\n",
    "    RMSE  = (np.sqrt(sum_sq_error / total_preds) if total_preds > 0 else 0.0)\n",
    "    WMAPE = (sum_abs_error / sum_gt) if sum_gt != 0 else 0.0\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1        = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0.0\n",
    "    iou       = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0.0\n",
    "    accuracy  = (TP + TN) / total_points if total_points > 0 else 0.0\n",
    "    cong_rate = total_gt_cong / total_points if total_points > 0 else 0.0\n",
    "\n",
    "    return dict(\n",
    "        # composed\n",
    "        MAE=MAE, RMSE=RMSE, WMAPE=WMAPE,\n",
    "        TP=TP, FP=FP, TN=TN, FN=FN,\n",
    "        precision=precision, recall=recall, f1=f1, iou=iou, accuracy=accuracy,\n",
    "        total_points=total_points,\n",
    "        gt_congested=total_gt_cong,\n",
    "        pred_congested=total_pred_cong,\n",
    "        congestion_rate=cong_rate,\n",
    "        alpha=alpha,\n",
    "        oracle_mode_type=oracle_mode_type,\n",
    "        # raw aggregates for global composition\n",
    "        _sum_abs_error=sum_abs_error,\n",
    "        _sum_sq_error=sum_sq_error,\n",
    "        _sum_gt=sum_gt,\n",
    "        _total_preds=total_preds,\n",
    "    ), used_vhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_congestion_oracle_metrics(stats_list):\n",
    "    import numpy as np\n",
    "\n",
    "    # Speed-error aggregates\n",
    "    total_sum_abs = sum(d.get(\"_sum_abs_error\", 0.0) for d in stats_list)\n",
    "    total_sum_sq  = sum(d.get(\"_sum_sq_error\",  0.0) for d in stats_list)\n",
    "    total_sum_gt  = sum(d.get(\"_sum_gt\",        0.0) for d in stats_list)\n",
    "    total_preds   = sum(d.get(\"_total_preds\",     0) for d in stats_list)\n",
    "\n",
    "    MAE  = (total_sum_abs / total_preds) if total_preds > 0 else 0.0\n",
    "    RMSE = (np.sqrt(total_sum_sq / total_preds) if total_preds > 0 else 0.0)\n",
    "    WMAPE = (total_sum_abs / total_sum_gt) if total_sum_gt != 0 else 0.0\n",
    "\n",
    "    # Classification/confusion aggregates\n",
    "    TP = sum(d[\"TP\"] for d in stats_list)\n",
    "    FP = sum(d[\"FP\"] for d in stats_list)\n",
    "    TN = sum(d[\"TN\"] for d in stats_list)\n",
    "    FN = sum(d[\"FN\"] for d in stats_list)\n",
    "    total_points = sum(d[\"total_points\"] for d in stats_list)\n",
    "    gt_cong = sum(d[\"gt_congested\"] for d in stats_list)\n",
    "    pred_cong = sum(d[\"pred_congested\"] for d in stats_list)\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1        = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0.0\n",
    "    iou       = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0.0\n",
    "    accuracy  = (TP + TN) / total_points if total_points > 0 else 0.0\n",
    "    cong_rate = gt_cong / total_points if total_points > 0 else 0.0\n",
    "\n",
    "    return dict(\n",
    "        # speed errors (composed like SCSR)\n",
    "        MAE=MAE, RMSE=RMSE, WMAPE=WMAPE,\n",
    "        # detection metrics\n",
    "        precision=precision, recall=recall, f1=f1, iou=iou, accuracy=accuracy,\n",
    "        congestion_rate=cong_rate,\n",
    "        # bookkeeping\n",
    "        total_points=total_points, gt_congested=gt_cong, pred_congested=pred_cong,\n",
    "        total_preds=total_preds,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cloudlet_pyg_new_metric_analysis(\n",
    "    data_iter,\n",
    "    big_err_threshold=20.0,\n",
    "    change_window=12,\n",
    "    change_delta=20.0,\n",
    "    change_tolerance=10.0,\n",
    "    cooldown=None,\n",
    "    oracle_mode_type=\"perfect\",\n",
    "):\n",
    "    if cooldown is None:\n",
    "        cooldown = max(1, change_window // 2)\n",
    "\n",
    "    # Aggregates (so we can compose across many calls)\n",
    "    sum_abs_error = 0.0\n",
    "    sum_sq_error  = 0.0\n",
    "    sum_gt        = 0.0\n",
    "    total_preds   = 0\n",
    "    big_err_count = 0\n",
    "\n",
    "    total_jam_events = 0\n",
    "    total_jam_hits   = 0\n",
    "    total_rec_events = 0\n",
    "    total_rec_hits   = 0\n",
    "\n",
    "    PERFECT_NOISE_ABS = min(3.06, change_tolerance * 0.49)  # always within tolerance\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in data_iter:\n",
    "            # accept (x, y) or just y\n",
    "            y = item[-1] if isinstance(item, (tuple, list)) else item\n",
    "            y_np = y.detach().cpu().numpy() if isinstance(y, torch.Tensor) else np.asarray(y)\n",
    "            y_np = np.asarray(y_np, dtype=np.float32)\n",
    "\n",
    "            # coerce to [T, N]\n",
    "            if y_np.ndim == 1:\n",
    "                y_np = y_np[:, None]\n",
    "            elif y_np.ndim > 2:\n",
    "                T = y_np.shape[0]\n",
    "                y_np = y_np.reshape(T, -1)\n",
    "\n",
    "            T, N = y_np.shape\n",
    "\n",
    "            # ---- Build predictions\n",
    "            if oracle_mode_type == \"perfect\":\n",
    "                # small noise strictly within tolerance -> always \"hit\" at event endpoints\n",
    "                noise = np.random.uniform(-PERFECT_NOISE_ABS, PERFECT_NOISE_ABS, size=y_np.shape).astype(np.float32)\n",
    "                y_pred_np = y_np + noise\n",
    "\n",
    "            elif oracle_mode_type == \"worst\":\n",
    "                y_pred_np = y_np.copy()\n",
    "                cool = np.zeros(N, dtype=int)\n",
    "                for t in range(1, T):\n",
    "                    cool = np.maximum(0, cool - 1)\n",
    "                    w_start = max(0, t - change_window)\n",
    "                    past = y_np[w_start:t, :]\n",
    "                    if past.size == 0:\n",
    "                        continue\n",
    "                    cur = y_np[t, :]\n",
    "                    jam_best = np.max(past - cur[None, :], axis=0)\n",
    "                    rec_best = np.max(cur[None, :] - past, axis=0)\n",
    "                    jam_mask = (jam_best >= change_delta) & (cool == 0)\n",
    "                    rec_mask = (rec_best >= change_delta) & (cool == 0)\n",
    "                    if not (jam_mask.any() or rec_mask.any()):\n",
    "                        continue\n",
    "                    idx_all = np.where(jam_mask | rec_mask)[0]\n",
    "                    if idx_all.size > 0:\n",
    "                        bump = change_tolerance + 1.0\n",
    "                        cur_sel = cur[idx_all]\n",
    "                        push_down = cur_sel > np.median(cur_sel)\n",
    "                        y_pred_np[t, idx_all[push_down]]  = cur_sel[push_down]  - bump\n",
    "                        y_pred_np[t, idx_all[~push_down]] = cur_sel[~push_down] + bump\n",
    "                        cool[idx_all] = np.maximum(cool[idx_all], cooldown)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown oracle_mode_type: {oracle_mode_type}\")\n",
    "\n",
    "            # ---- Base metrics\n",
    "            d = np.abs(y_np - y_pred_np)\n",
    "            sum_abs_error += float(np.sum(d))\n",
    "            sum_sq_error  += float(np.sum(d ** 2))\n",
    "            sum_gt        += float(np.sum(y_np))\n",
    "            big_err_count += int(np.sum(d >= big_err_threshold))\n",
    "            total_preds   += d.size\n",
    "\n",
    "            # ---- Event detection + hits\n",
    "            cool = np.zeros(N, dtype=int)\n",
    "            for t in range(1, T):\n",
    "                cool = np.maximum(0, cool - 1)\n",
    "                w_start = max(0, t - change_window)\n",
    "                past = y_np[w_start:t, :]\n",
    "                if past.size == 0:\n",
    "                    continue\n",
    "\n",
    "                cur = y_np[t, :]\n",
    "                pred_cur = y_pred_np[t, :]\n",
    "\n",
    "                jam_best = np.max(past - cur[None, :], axis=0)\n",
    "                rec_best = np.max(cur[None, :] - past, axis=0)\n",
    "\n",
    "                jam_mask = (jam_best >= change_delta) & (cool == 0)\n",
    "                rec_mask = (rec_best >= change_delta) & (cool == 0)\n",
    "\n",
    "                if not (jam_mask.any() or rec_mask.any()):\n",
    "                    continue\n",
    "\n",
    "                abs_err = np.abs(pred_cur - cur)\n",
    "\n",
    "                if jam_mask.any():\n",
    "                    idx = np.where(jam_mask)[0]\n",
    "                    total_jam_events += idx.size\n",
    "                    total_jam_hits   += int(np.sum(abs_err[idx] <= change_tolerance))\n",
    "                    cool[idx] = np.maximum(cool[idx], cooldown)\n",
    "\n",
    "                if rec_mask.any():\n",
    "                    idx = np.where(rec_mask)[0]\n",
    "                    total_rec_events += idx.size\n",
    "                    total_rec_hits   += int(np.sum(abs_err[idx] <= change_tolerance))\n",
    "                    cool[idx] = np.maximum(cool[idx], cooldown)\n",
    "\n",
    "    # Derived metrics for this call\n",
    "    MAE  = (sum_abs_error / total_preds) if total_preds > 0 else 0.0\n",
    "    RMSE = (np.sqrt(sum_sq_error / total_preds) if total_preds > 0 else 0.0)\n",
    "    WMAPE = (sum_abs_error / sum_gt) if sum_gt != 0 else 0.0\n",
    "    BIG_ERR_RATE = (big_err_count / total_preds) if total_preds > 0 else 0.0\n",
    "\n",
    "    sudden_events = total_jam_events + total_rec_events\n",
    "    sudden_hits   = total_jam_hits   + total_rec_hits\n",
    "    SUDDEN_EVENT_RATE = (sudden_hits / sudden_events) if sudden_events > 0 else 0.0\n",
    "    JAM_EVENT_RATE    = (total_jam_hits / total_jam_events) if total_jam_events > 0 else 0.0\n",
    "    REC_EVENT_RATE    = (total_rec_hits / total_rec_events) if total_rec_events > 0 else 0.0\n",
    "\n",
    "    # Return both the composed metrics and the raw aggregates (to combine across epochs)\n",
    "    return dict(\n",
    "        # composed\n",
    "        MAE=MAE, RMSE=RMSE, WMAPE=WMAPE,\n",
    "        big_err_count=big_err_count, big_err_rate=BIG_ERR_RATE,\n",
    "        sudden_event_count=sudden_events, sudden_event_hits=sudden_hits, sudden_event_rate=SUDDEN_EVENT_RATE,\n",
    "        jam_event_count=total_jam_events, jam_event_hits=total_jam_hits, jam_event_rate=JAM_EVENT_RATE,\n",
    "        rec_event_count=total_rec_events, rec_event_hits=total_rec_hits, rec_event_rate=REC_EVENT_RATE,\n",
    "        # aggregates\n",
    "        _sum_abs_error=sum_abs_error,\n",
    "        _sum_sq_error=sum_sq_error,\n",
    "        _sum_gt=sum_gt,\n",
    "        _total_preds=total_preds,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_final_metrics(aggregate_list):\n",
    "    # aggregate_list is a list of dicts returned by evaluate_cloudlet_pyg_new_metric_analysis_v2\n",
    "    total_sum_abs = sum(d[\"_sum_abs_error\"] for d in aggregate_list)\n",
    "    total_sum_sq  = sum(d[\"_sum_sq_error\"]  for d in aggregate_list)\n",
    "    total_sum_gt  = sum(d[\"_sum_gt\"]        for d in aggregate_list)\n",
    "    total_preds   = sum(d[\"_total_preds\"]   for d in aggregate_list)\n",
    "\n",
    "    total_jam_events = sum(d[\"jam_event_count\"] for d in aggregate_list)\n",
    "    total_jam_hits   = sum(d[\"jam_event_hits\"]  for d in aggregate_list)\n",
    "    total_rec_events = sum(d[\"rec_event_count\"] for d in aggregate_list)\n",
    "    total_rec_hits   = sum(d[\"rec_event_hits\"]  for d in aggregate_list)\n",
    "\n",
    "    big_err_count    = sum(d[\"big_err_count\"]   for d in aggregate_list)\n",
    "\n",
    "    MAE  = (total_sum_abs / total_preds) if total_preds > 0 else 0.0\n",
    "    RMSE = (np.sqrt(total_sum_sq / total_preds) if total_preds > 0 else 0.0)\n",
    "    WMAPE = (total_sum_abs / total_sum_gt) if total_sum_gt != 0 else 0.0\n",
    "    BIG_ERR_RATE = (big_err_count / total_preds) if total_preds > 0 else 0.0\n",
    "\n",
    "    sudden_events = total_jam_events + total_rec_events\n",
    "    sudden_hits   = total_jam_hits   + total_rec_hits\n",
    "    SUDDEN_EVENT_RATE = (sudden_hits / sudden_events) if sudden_events > 0 else 0.0\n",
    "    JAM_EVENT_RATE    = (total_jam_hits / total_jam_events) if total_jam_events > 0 else 0.0\n",
    "    REC_EVENT_RATE    = (total_rec_hits / total_rec_events) if total_rec_events > 0 else 0.0\n",
    "\n",
    "    return dict(\n",
    "        MAE=MAE, RMSE=RMSE, WMAPE=WMAPE, BIG_ERR_RATE=BIG_ERR_RATE,\n",
    "        SUDDEN_EVENT_RATE=SUDDEN_EVENT_RATE,\n",
    "        JAM_EVENT_RATE=JAM_EVENT_RATE,\n",
    "        REC_EVENT_RATE=REC_EVENT_RATE,\n",
    "        total_preds=total_preds, sudden_events=sudden_events\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_congestion_alpha_propagation_combined(\n",
    "        v,                      # [T, N] speeds (np or torch)\n",
    "        edge_index,             # [2, E] torch.LongTensor or np.array of edges (0-based)\n",
    "        alpha=0.5,\n",
    "        vhat=None,\n",
    "        use_nanmedian=True,\n",
    "        require_both_when_deg2=True,   # if deg(i)==2, require both neighbors; else use \">=2 neighbors\" rule\n",
    "    ):\n",
    "    # to numpy\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        v = v.detach().cpu().numpy()\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    if v.ndim == 1:\n",
    "        v = v[:, None]\n",
    "    elif v.ndim > 2:\n",
    "        v = v.reshape(v.shape[0], -1)\n",
    "    T, N = v.shape\n",
    "\n",
    "    # medians\n",
    "    if vhat is None:\n",
    "        vhat = (np.nanmedian(v, axis=0) if use_nanmedian else np.median(np.nan_to_num(v, nan=0.0), axis=0))\n",
    "    else:\n",
    "        vhat = np.asarray(vhat, dtype=np.float32)\n",
    "        assert vhat.shape == (N,), f\"vhat must be (N,), got {vhat.shape}\"\n",
    "\n",
    "    thr = alpha * vhat\n",
    "    thr_full = thr[None, :].repeat(T, axis=0)\n",
    "    base = (v < thr_full)  # Rule 1\n",
    "\n",
    "    # build undirected adjacency lists\n",
    "    if isinstance(edge_index, torch.Tensor):\n",
    "        ei = edge_index.detach().cpu().numpy()\n",
    "    else:\n",
    "        ei = np.asarray(edge_index)\n",
    "    assert ei.shape[0] == 2, \"edge_index must be shape [2, E]\"\n",
    "    nbrs = defaultdict(list)\n",
    "    for u, w in ei.T:\n",
    "        nbrs[int(u)].append(int(w))\n",
    "        nbrs[int(w)].append(int(u))  # undirected for “immediate neighbors”\n",
    "\n",
    "    # Rule 2 graph-aware spatial continuity\n",
    "    spatial = np.zeros_like(base, dtype=bool)\n",
    "    for i in range(N):\n",
    "        neigh = nbrs.get(i, [])\n",
    "        deg = len(neigh)\n",
    "        if deg < 2:\n",
    "            continue\n",
    "        # congested neighbors at time t\n",
    "        # base[:, neigh] shape [T, deg] -> count how many neighbors are below threshold\n",
    "        cong_neigh_count = np.sum(base[:, neigh], axis=1)  # [T]\n",
    "\n",
    "        if require_both_when_deg2 and deg == 2:\n",
    "            spatial[:, i] = (cong_neigh_count == 2)\n",
    "        else:\n",
    "            spatial[:, i] = (cong_neigh_count >= 2)\n",
    "\n",
    "    # Rule 3 temporal continuity (same as before)\n",
    "    temporal = np.zeros_like(base, dtype=bool)\n",
    "    if T >= 3:\n",
    "        prev_t = base[:-2, :]\n",
    "        next_t = base[ 2:, :]\n",
    "        temporal[1:-1, :] = prev_t & next_t\n",
    "\n",
    "    congested = base | spatial | temporal\n",
    "    return congested, vhat\n",
    "\n",
    "def evaluate_oracle_scsr_and_alpha_combined(\n",
    "    data_iter,\n",
    "    edge_index,\n",
    "    *,\n",
    "    # SCSR params\n",
    "    big_err_threshold=20.0,\n",
    "    change_window=12,\n",
    "    change_delta=20.0,\n",
    "    change_tolerance=10.0,\n",
    "    cooldown=None,\n",
    "    # Oracle choice for BOTH SCSR + α-propagation\n",
    "    oracle_mode_type=\"perfect\",  # \"perfect\" | \"worst\"\n",
    "    perfect_noise_abs=None,      # default will be min(3.06, change_tolerance*0.49)\n",
    "    # α-propagation params\n",
    "    alpha=0.5,\n",
    "    vhat=None,                   # pass persistent medians\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds y_pred using **SCSR oracle** logic. Then:\n",
    "      - computes base errors & SCSR event hits,\n",
    "      - runs α-based congestion on GT vs y_pred, and returns confusion metrics too.\n",
    "\n",
    "    Returns (metrics_dict, used_vhat).\n",
    "    metrics_dict includes BOTH SCSR & α metrics and raw aggregates for global composition.\n",
    "    \"\"\"\n",
    "    if cooldown is None:\n",
    "        cooldown = max(1, change_window // 2)\n",
    "\n",
    "    if perfect_noise_abs is None:\n",
    "        perfect_noise_abs = min(3.06, change_tolerance * 0.49)  # ensures SCSR hits at endpoints\n",
    "\n",
    "    # ---------- global aggregates ----------\n",
    "    # base errors\n",
    "    sum_abs_error = 0.0\n",
    "    sum_sq_error  = 0.0\n",
    "    sum_gt        = 0.0\n",
    "    total_preds   = 0\n",
    "    big_err_count = 0\n",
    "\n",
    "    # SCSR events\n",
    "    total_jam_events = 0\n",
    "    total_jam_hits   = 0\n",
    "    total_rec_events = 0\n",
    "    total_rec_hits   = 0\n",
    "\n",
    "    # α-propagation confusion\n",
    "    TP = FP = TN = FN = 0\n",
    "    total_points = 0\n",
    "    total_gt_cong = 0\n",
    "    total_pred_cong = 0\n",
    "\n",
    "    used_vhat = vhat\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in data_iter:\n",
    "            # accept (x, y) or just y\n",
    "            y = item[-1] if isinstance(item, (tuple, list)) else item\n",
    "            y_np = y.detach().cpu().numpy() if isinstance(y, torch.Tensor) else np.asarray(y)\n",
    "            y_np = np.asarray(y_np, dtype=np.float32)\n",
    "\n",
    "            # shape -> [T, N]\n",
    "            if y_np.ndim == 1:\n",
    "                y_np = y_np[:, None]\n",
    "            elif y_np.ndim > 2:\n",
    "                T = y_np.shape[0]\n",
    "                y_np = y_np.reshape(T, -1)\n",
    "\n",
    "            T, N = y_np.shape\n",
    "\n",
    "            # ---------- Build y_pred via SCSR oracle ----------\n",
    "            if oracle_mode_type == \"perfect\":\n",
    "                # Add small noise, but keep always within change_tolerance around GT so SCSR hits are guaranteed.\n",
    "                noise = np.random.uniform(-perfect_noise_abs, perfect_noise_abs, size=y_np.shape).astype(np.float32)\n",
    "                y_pred_np = y_np + noise\n",
    "\n",
    "            elif oracle_mode_type == \"worst\":\n",
    "                # Copy GT, but at SCSR event endpoints force error > change_tolerance\n",
    "                y_pred_np = y_np.copy()\n",
    "                cool = np.zeros(N, dtype=int)\n",
    "                for t in range(1, T):\n",
    "                    cool = np.maximum(0, cool - 1)\n",
    "                    w_start = max(0, t - change_window)\n",
    "                    past = y_np[w_start:t, :]\n",
    "                    if past.size == 0:\n",
    "                        continue\n",
    "                    cur = y_np[t, :]\n",
    "                    jam_best = np.max(past - cur[None, :], axis=0)\n",
    "                    rec_best = np.max(cur[None, :] - past, axis=0)\n",
    "                    jam_mask = (jam_best >= change_delta) & (cool == 0)\n",
    "                    rec_mask = (rec_best >= change_delta) & (cool == 0)\n",
    "                    if not (jam_mask.any() or rec_mask.any()):\n",
    "                        continue\n",
    "                    idx_all = np.where(jam_mask | rec_mask)[0]\n",
    "                    if idx_all.size > 0:\n",
    "                        bump = change_tolerance + 1.0\n",
    "                        cur_sel = cur[idx_all]\n",
    "                        push_down = cur_sel > np.median(cur_sel)\n",
    "                        y_pred_np[t, idx_all[push_down]]  = cur_sel[push_down]  - bump\n",
    "                        y_pred_np[t, idx_all[~push_down]] = cur_sel[~push_down] + bump\n",
    "                        cool[idx_all] = np.maximum(cool[idx_all], cooldown)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown oracle_mode_type: {oracle_mode_type}\")\n",
    "\n",
    "            # ---------- Base errors ----------\n",
    "            d = np.abs(y_np - y_pred_np)\n",
    "            sum_abs_error += float(np.sum(d))\n",
    "            sum_sq_error  += float(np.sum(d ** 2))\n",
    "            sum_gt        += float(np.sum(y_np))\n",
    "            big_err_count += int(np.sum(d >= big_err_threshold))\n",
    "            total_preds   += d.size\n",
    "\n",
    "            # ---------- SCSR event detection + hits ----------\n",
    "            cool = np.zeros(N, dtype=int)\n",
    "            for t in range(1, T):\n",
    "                cool = np.maximum(0, cool - 1)\n",
    "                w_start = max(0, t - change_window)\n",
    "                past = y_np[w_start:t, :]\n",
    "                if past.size == 0:\n",
    "                    continue\n",
    "\n",
    "                cur = y_np[t, :]\n",
    "                pred_cur = y_pred_np[t, :]\n",
    "\n",
    "                jam_best = np.max(past - cur[None, :], axis=0)\n",
    "                rec_best = np.max(cur[None, :] - past, axis=0)\n",
    "\n",
    "                jam_mask = (jam_best >= change_delta) & (cool == 0)\n",
    "                rec_mask = (rec_best >= change_delta) & (cool == 0)\n",
    "                if not (jam_mask.any() or rec_mask.any()):\n",
    "                    continue\n",
    "\n",
    "                abs_err = np.abs(pred_cur - cur)\n",
    "\n",
    "                if jam_mask.any():\n",
    "                    idx = np.where(jam_mask)[0]\n",
    "                    total_jam_events += idx.size\n",
    "                    total_jam_hits   += int(np.sum(abs_err[idx] <= change_tolerance))\n",
    "                    cool[idx] = np.maximum(cool[idx], cooldown)\n",
    "\n",
    "                if rec_mask.any():\n",
    "                    idx = np.where(rec_mask)[0]\n",
    "                    total_rec_events += idx.size\n",
    "                    total_rec_hits   += int(np.sum(abs_err[idx] <= change_tolerance))\n",
    "                    cool[idx] = np.maximum(cool[idx], cooldown)\n",
    "\n",
    "            # ---------- α-based congestion: GT vs y_pred ----------\n",
    "            gt_mask, used_vhat = detect_congestion_alpha_propagation_combined(y_np, edge_index, alpha=alpha, vhat=used_vhat)\n",
    "            pr_mask, _         = detect_congestion_alpha_propagation_combined(y_pred_np, edge_index, alpha=alpha, vhat=used_vhat)\n",
    "\n",
    "            gt = gt_mask.reshape(-1)\n",
    "            pr = pr_mask.reshape(-1)\n",
    "\n",
    "            tp = int(np.sum(gt & pr))\n",
    "            tn = int(np.sum(~gt & ~pr))\n",
    "            fp = int(np.sum(~gt & pr))\n",
    "            fn = int(np.sum(gt & ~pr))\n",
    "\n",
    "            TP += tp; TN += tn; FP += fp; FN += fn\n",
    "            total_points   += gt.size\n",
    "            total_gt_cong  += int(np.sum(gt))\n",
    "            total_pred_cong+= int(np.sum(pr))\n",
    "\n",
    "    # ---------- composed metrics ----------\n",
    "    # Base errors\n",
    "    MAE   = (sum_abs_error / total_preds) if total_preds > 0 else 0.0\n",
    "    RMSE  = (np.sqrt(sum_sq_error / total_preds) if total_preds > 0 else 0.0)\n",
    "    WMAPE = (sum_abs_error / sum_gt) if sum_gt != 0 else 0.0\n",
    "    BIG_ERR_RATE = (big_err_count / total_preds) if total_preds > 0 else 0.0\n",
    "\n",
    "    # SCSR\n",
    "    sudden_events = total_jam_events + total_rec_events\n",
    "    sudden_hits   = total_jam_hits   + total_rec_hits\n",
    "    SUDDEN_EVENT_RATE = (sudden_hits / sudden_events) if sudden_events > 0 else 0.0\n",
    "    JAM_EVENT_RATE    = (total_jam_hits / total_jam_events) if total_jam_events > 0 else 0.0\n",
    "    REC_EVENT_RATE    = (total_rec_hits / total_rec_events) if total_rec_events > 0 else 0.0\n",
    "\n",
    "    # α-propagation detection metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1        = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0.0\n",
    "    iou       = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0.0\n",
    "    accuracy  = (TP + TN) / total_points if total_points > 0 else 0.0\n",
    "    cong_rate = total_gt_cong / total_points if total_points > 0 else 0.0\n",
    "\n",
    "    return dict(\n",
    "        # ---- base errors (composed)\n",
    "        MAE=MAE, RMSE=RMSE, WMAPE=WMAPE, BIG_ERR_RATE=BIG_ERR_RATE,\n",
    "\n",
    "        # ---- SCSR composed\n",
    "        sudden_event_count=sudden_events,\n",
    "        sudden_event_hits=sudden_hits,\n",
    "        sudden_event_rate=SUDDEN_EVENT_RATE,\n",
    "        jam_event_count=total_jam_events, jam_event_hits=total_jam_hits, jam_event_rate=JAM_EVENT_RATE,\n",
    "        rec_event_count=total_rec_events, rec_event_hits=total_rec_hits, rec_event_rate=REC_EVENT_RATE,\n",
    "\n",
    "        # ---- α-propagation composed\n",
    "        TP=TP, FP=FP, TN=TN, FN=FN,\n",
    "        precision=precision, recall=recall, f1=f1, iou=iou, accuracy=accuracy,\n",
    "        gt_congested=total_gt_cong, pred_congested=total_pred_cong,\n",
    "        congestion_rate=cong_rate,\n",
    "        alpha=alpha,\n",
    "        oracle_mode_type=oracle_mode_type,\n",
    "\n",
    "        # ---- raw aggregates for global composition\n",
    "        _sum_abs_error=sum_abs_error,\n",
    "        _sum_sq_error=sum_sq_error,\n",
    "        _sum_gt=sum_gt,\n",
    "        _total_preds=total_preds,\n",
    "    ), used_vhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_combined(stats_list):\n",
    "    import numpy as np\n",
    "\n",
    "    # base errors\n",
    "    total_sum_abs = sum(d[\"_sum_abs_error\"] for d in stats_list)\n",
    "    total_sum_sq  = sum(d[\"_sum_sq_error\"]  for d in stats_list)\n",
    "    total_sum_gt  = sum(d[\"_sum_gt\"]        for d in stats_list)\n",
    "    total_preds   = sum(d[\"_total_preds\"]   for d in stats_list)\n",
    "\n",
    "    MAE   = (total_sum_abs / total_preds) if total_preds > 0 else 0.0\n",
    "    RMSE  = (np.sqrt(total_sum_sq / total_preds) if total_preds > 0 else 0.0)\n",
    "    WMAPE = (total_sum_abs / total_sum_gt) if total_sum_gt != 0 else 0.0\n",
    "\n",
    "    # SCSR\n",
    "    jam_events = sum(d[\"jam_event_count\"] for d in stats_list)\n",
    "    jam_hits   = sum(d[\"jam_event_hits\"]  for d in stats_list)\n",
    "    rec_events = sum(d[\"rec_event_count\"] for d in stats_list)\n",
    "    rec_hits   = sum(d[\"rec_event_hits\"]  for d in stats_list)\n",
    "    sudden_events = jam_events + rec_events\n",
    "    sudden_hits   = jam_hits   + rec_hits\n",
    "\n",
    "    SUDDEN_EVENT_RATE = (sudden_hits / sudden_events) if sudden_events > 0 else 0.0\n",
    "    JAM_EVENT_RATE    = (jam_hits / jam_events) if jam_events > 0 else 0.0\n",
    "    REC_EVENT_RATE    = (rec_hits / rec_events) if rec_events > 0 else 0.0\n",
    "\n",
    "    # α-propagation confusion\n",
    "    TP = sum(d[\"TP\"] for d in stats_list)\n",
    "    FP = sum(d[\"FP\"] for d in stats_list)\n",
    "    TN = sum(d[\"TN\"] for d in stats_list)\n",
    "    FN = sum(d[\"FN\"] for d in stats_list)\n",
    "    total_points = TP + FP + TN + FN\n",
    "    gt_cong = sum(d[\"gt_congested\"] for d in stats_list)\n",
    "    pred_cong = sum(d[\"pred_congested\"] for d in stats_list)\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recall    = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    f1        = 2*precision*recall/(precision+recall) if (precision+recall) > 0 else 0.0\n",
    "    iou       = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0.0\n",
    "    accuracy  = (TP + TN) / total_points if total_points > 0 else 0.0\n",
    "    cong_rate = gt_cong / total_points if total_points > 0 else 0.0\n",
    "\n",
    "    return dict(\n",
    "        # base errors\n",
    "        MAE=MAE, RMSE=RMSE, WMAPE=WMAPE,\n",
    "        total_preds=total_preds,\n",
    "\n",
    "        # SCSR\n",
    "        SUDDEN_EVENT_RATE=SUDDEN_EVENT_RATE,\n",
    "        JAM_EVENT_RATE=JAM_EVENT_RATE,\n",
    "        REC_EVENT_RATE=REC_EVENT_RATE,\n",
    "        sudden_events=sudden_events,\n",
    "\n",
    "        # α-propagation\n",
    "        precision=precision, recall=recall, f1=f1, iou=iou, accuracy=accuracy,\n",
    "        congestion_rate=cong_rate,\n",
    "        gt_congested=gt_cong, pred_congested=pred_cong,\n",
    "        total_points=total_points\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_train(epoch, edge_index, x_train, y_train, val_iter, num_epochs, oracle_mode_type, agg, used_vhat, agg_stats, agg_combined):\n",
    "    if epoch < num_epochs:\n",
    "        # Create masked validation dataset\n",
    "        new_x_val = x_train[\n",
    "            end_of_initial_data_index + (data_per_step * (epoch)):\n",
    "            end_of_initial_data_index + (data_per_step * (epoch + 1))\n",
    "            ]\n",
    "        new_y_val = y_train[\n",
    "            end_of_initial_data_index + (data_per_step * (epoch)):\n",
    "            end_of_initial_data_index + (data_per_step * (epoch + 1))\n",
    "        ]\n",
    "        new_val_data = utils.data.TensorDataset(new_x_val, new_y_val)\n",
    "        new_val_iter = utils.data.DataLoader(dataset=new_val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # res = evaluate_cloudlet_pyg_new_metric_analysis(\n",
    "        #     new_val_iter, oracle_mode_type=oracle_mode_type\n",
    "        # )\n",
    "        # stats_cong, used_vhat = evaluate_congestion_alpha_with_oracle(\n",
    "        #     new_val_iter,\n",
    "        #     alpha=0.5, vhat=used_vhat,\n",
    "        #     oracle_mode_type=oracle_mode_type,\n",
    "        # )\n",
    "        res_combined, used_vhat = evaluate_oracle_scsr_and_alpha_combined(\n",
    "            new_val_iter,\n",
    "            edge_index=edge_index,\n",
    "            oracle_mode_type=oracle_mode_type,   # or \"perfect\"\n",
    "            change_window=12, change_delta=20.0, change_tolerance=10.0,\n",
    "            alpha=0.5, vhat=used_vhat\n",
    "        )\n",
    "\n",
    "        # agg_stats.append(stats_cong)\n",
    "        # agg.append(res)\n",
    "        agg_combined.append(res_combined)\n",
    "    elif epoch == num_epochs:\n",
    "        res_combined, used_vhat = evaluate_oracle_scsr_and_alpha_combined(\n",
    "            val_iter,\n",
    "            edge_index=edge_index,\n",
    "            oracle_mode_type=oracle_mode_type,   # or \"perfect\"\n",
    "            change_window=12, change_delta=20.0, change_tolerance=10.0,\n",
    "            alpha=0.5, vhat=used_vhat\n",
    "        )\n",
    "        agg_combined.append(res_combined)\n",
    "        # res = evaluate_cloudlet_pyg_new_metric_analysis(\n",
    "        #     val_iter, oracle_mode_type=oracle_mode_type\n",
    "        # )\n",
    "        # stats_cong, used_vhat = evaluate_congestion_alpha_with_oracle(\n",
    "        #     val_iter,\n",
    "        #     alpha=0.5, vhat=used_vhat,\n",
    "        #     oracle_mode_type=oracle_mode_type,\n",
    "        # )\n",
    "\n",
    "        # agg_stats.append(stats_cong)\n",
    "        # agg.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End index and data step correctly selected!\n"
     ]
    }
   ],
   "source": [
    "adj = sp.load_npz(adj_path)\n",
    "adj = adj.tocsc()\n",
    "edge_index, _ = from_scipy_sparse_matrix(adj)\n",
    "\n",
    "data_col = pd.read_csv(dataset_path).shape[0]\n",
    "\n",
    "len_val = int(math.floor(data_col * val_rate))\n",
    "len_test = int(math.floor(data_col * test_rate))\n",
    "len_train = int(data_col - len_val - len_test)\n",
    "\n",
    "if ((len_train - end_of_initial_data_index) % data_per_step == 0):\n",
    "    print(\"End index and data step correctly selected!\")\n",
    "else:\n",
    "    print(f\"End index and data step WRONGLY selected: {(len_train - end_of_initial_data_index) % data_per_step}\")\n",
    "    exit(1)\n",
    "\n",
    "train, val, test = load_data(len_train, len_val)\n",
    "train, val, test = zscore_preprocess_2d_data(train.values, val.values, test.values)\n",
    "\n",
    "x_train, y_train = data_transform(train, n_his, n_pred)\n",
    "x_val, y_val = data_transform(val, n_his, n_pred)\n",
    "# x_test, y_test = data_transform(test, n_his, n_pred)\n",
    "\n",
    "train_data = utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = utils.data.TensorDataset(x_val, y_val)\n",
    "val_iter = utils.data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False)\n",
    "# test_data = utils.data.TensorDataset(x_test, y_test)\n",
    "# test_iter = utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MAE: 0.0\n",
      "FINAL RMSE: 0.0\n",
      "FINAL WMAPE: 0.0\n",
      "FINAL SUDDEN_EVENT_RATE: 0.0\n",
      "\n",
      "======== α-based Congestion — FINAL ========\n",
      "MAE:   0.000000\n",
      "RMSE:  0.000000\n",
      "WMAPE: 0.000000\n",
      "Recall (hit rate): 0.000000\n",
      "Precision:         0.000000\n",
      "F1:                0.000000\n",
      "IoU:               0.000000\n",
      "Accuracy:          0.000000\n",
      "GT congestion rate:0.000000\n",
      "Total points:      0\n",
      "Total preds:       0\n",
      "=== Combined (shared y_pred) ===\n",
      "MAE=0.088944  RMSE=0.989131  WMAPE=0.001423\n",
      "SCSR:  sudden_event_rate=0.000000  jam_rate=0.000000  rec_rate=0.000000\n",
      "ALPHA: recall=0.990698 precision=0.989271 F1=0.989984 IoU=0.980166 accuracy=0.993063\n",
      "ALPHA: gt_congestion_rate=0.346045  gt_cong=2505152  pred_cong=2508766\n"
     ]
    }
   ],
   "source": [
    "agg = []\n",
    "agg_stats = []\n",
    "agg_combined = []\n",
    "used_vhat = None\n",
    "num_epochs = int((len_train - end_of_initial_data_index) / data_per_step)\n",
    "\n",
    "# for epoch in range(0, num_epochs + 1): \n",
    "#     online_train(epoch, x_train, y_train, val_iter, num_epochs, oracle_mode_type=\"perfect\", agg=agg)\n",
    "\n",
    "for epoch in range(0, num_epochs + 1): \n",
    "    online_train(epoch, edge_index, x_train, y_train, val_iter, num_epochs, oracle_mode_type=\"worst\", agg=agg, used_vhat=used_vhat, agg_stats=agg_stats, agg_combined=agg_combined)\n",
    "\n",
    "final_metrics = aggregate_final_metrics(agg)\n",
    "print(\"FINAL MAE:\", final_metrics[\"MAE\"])\n",
    "print(\"FINAL RMSE:\", final_metrics[\"RMSE\"])\n",
    "print(\"FINAL WMAPE:\", final_metrics[\"WMAPE\"])\n",
    "print(\"FINAL SUDDEN_EVENT_RATE:\", final_metrics[\"SUDDEN_EVENT_RATE\"])\n",
    "\n",
    "final = aggregate_congestion_oracle_metrics(agg_stats)\n",
    "print(\"\\n======== α-based Congestion — FINAL ========\")\n",
    "print(f\"MAE:   {final['MAE']:.6f}\")\n",
    "print(f\"RMSE:  {final['RMSE']:.6f}\")\n",
    "print(f\"WMAPE: {final['WMAPE']:.6f}\")\n",
    "print(f\"Recall (hit rate): {final['recall']:.6f}\")\n",
    "print(f\"Precision:         {final['precision']:.6f}\")\n",
    "print(f\"F1:                {final['f1']:.6f}\")\n",
    "print(f\"IoU:               {final['iou']:.6f}\")\n",
    "print(f\"Accuracy:          {final['accuracy']:.6f}\")\n",
    "print(f\"GT congestion rate:{final['congestion_rate']:.6f}\")\n",
    "print(f\"Total points:      {final['total_points']}\")\n",
    "print(f\"Total preds:       {final['total_preds']}\")\n",
    "\n",
    "final_combined = aggregate_combined(agg_combined)\n",
    "print(\"=== Combined (shared y_pred) ===\")\n",
    "print(f\"MAE={final_combined['MAE']:.6f}  RMSE={final_combined['RMSE']:.6f}  WMAPE={final_combined['WMAPE']:.6f}\")\n",
    "print(f\"SCSR:  sudden_event_rate={final_combined['SUDDEN_EVENT_RATE']:.6f}  \"\n",
    "      f\"jam_rate={final_combined['JAM_EVENT_RATE']:.6f}  rec_rate={final_combined['REC_EVENT_RATE']:.6f}\")\n",
    "print(f\"ALPHA: recall={final_combined['recall']:.6f} precision={final_combined['precision']:.6f} \"\n",
    "      f\"F1={final_combined['f1']:.6f} IoU={final_combined['iou']:.6f} accuracy={final_combined['accuracy']:.6f}\")\n",
    "print(f\"ALPHA: gt_congestion_rate={final_combined['congestion_rate']:.6f}  \"\n",
    "      f\"gt_cong={final_combined['gt_congested']}  pred_cong={final_combined['pred_congested']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MAE: 0.0\n",
      "FINAL RMSE: 0.0\n",
      "FINAL WMAPE: 0.0\n",
      "FINAL SUDDEN_EVENT_RATE: 0.0\n",
      "\n",
      "======== α-based Congestion — FINAL ========\n",
      "MAE:   0.000000\n",
      "RMSE:  0.000000\n",
      "WMAPE: 0.000000\n",
      "Recall (hit rate): 0.000000\n",
      "Precision:         0.000000\n",
      "F1:                0.000000\n",
      "IoU:               0.000000\n",
      "Accuracy:          0.000000\n",
      "GT congestion rate:0.000000\n",
      "Total points:      0\n",
      "Total preds:       0\n",
      "=== Combined (shared y_pred) ===\n",
      "MAE=1.530372  RMSE=1.767036  WMAPE=0.024488\n",
      "SCSR:  sudden_event_rate=1.000000  jam_rate=1.000000  rec_rate=1.000000\n",
      "ALPHA: recall=0.984189 precision=0.977885 F1=0.981027 IoU=0.962761 accuracy=0.986827\n",
      "ALPHA: gt_congestion_rate=0.346045  gt_cong=2505152  pred_cong=2521302\n"
     ]
    }
   ],
   "source": [
    "agg = []\n",
    "used_vhat = None\n",
    "agg_stats = []\n",
    "agg_combined = []\n",
    "num_epochs = int((len_train - end_of_initial_data_index) / data_per_step)\n",
    "\n",
    "for epoch in range(0, num_epochs + 1): \n",
    "    online_train(epoch, edge_index, x_train, y_train, val_iter, num_epochs, oracle_mode_type=\"perfect\", agg=agg, used_vhat=used_vhat, agg_stats=agg_stats, agg_combined=agg_combined)\n",
    "\n",
    "final_metrics = aggregate_final_metrics(agg)\n",
    "print(\"FINAL MAE:\", final_metrics[\"MAE\"])\n",
    "print(\"FINAL RMSE:\", final_metrics[\"RMSE\"])\n",
    "print(\"FINAL WMAPE:\", final_metrics[\"WMAPE\"])\n",
    "print(\"FINAL SUDDEN_EVENT_RATE:\", final_metrics[\"SUDDEN_EVENT_RATE\"])\n",
    "\n",
    "final = aggregate_congestion_oracle_metrics(agg_stats)\n",
    "print(\"\\n======== α-based Congestion — FINAL ========\")\n",
    "print(f\"MAE:   {final['MAE']:.6f}\")\n",
    "print(f\"RMSE:  {final['RMSE']:.6f}\")\n",
    "print(f\"WMAPE: {final['WMAPE']:.6f}\")\n",
    "print(f\"Recall (hit rate): {final['recall']:.6f}\")\n",
    "print(f\"Precision:         {final['precision']:.6f}\")\n",
    "print(f\"F1:                {final['f1']:.6f}\")\n",
    "print(f\"IoU:               {final['iou']:.6f}\")\n",
    "print(f\"Accuracy:          {final['accuracy']:.6f}\")\n",
    "print(f\"GT congestion rate:{final['congestion_rate']:.6f}\")\n",
    "print(f\"Total points:      {final['total_points']}\")\n",
    "print(f\"Total preds:       {final['total_preds']}\")\n",
    "\n",
    "final_combined = aggregate_combined(agg_combined)\n",
    "print(\"=== Combined (shared y_pred) ===\")\n",
    "print(f\"MAE={final_combined['MAE']:.6f}  RMSE={final_combined['RMSE']:.6f}  WMAPE={final_combined['WMAPE']:.6f}\")\n",
    "print(f\"SCSR:  sudden_event_rate={final_combined['SUDDEN_EVENT_RATE']:.6f}  \"\n",
    "      f\"jam_rate={final_combined['JAM_EVENT_RATE']:.6f}  rec_rate={final_combined['REC_EVENT_RATE']:.6f}\")\n",
    "print(f\"ALPHA: recall={final_combined['recall']:.6f} precision={final_combined['precision']:.6f} \"\n",
    "      f\"F1={final_combined['f1']:.6f} IoU={final_combined['iou']:.6f} accuracy={final_combined['accuracy']:.6f}\")\n",
    "print(f\"ALPHA: gt_congestion_rate={final_combined['congestion_rate']:.6f}  \"\n",
    "      f\"gt_cong={final_combined['gt_congested']}  pred_cong={final_combined['pred_congested']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stgcn-distributed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
